---
title: "sm2732_BF_HW9"
output: html_document
date: "2022-11-11"
---


```{r}

# Business Forecasting HW9
# Name - Sarjak Atul Maniar
# Email - sm2732@scarletmail.rutgers.edu


library(fpp)
library(fpp2)
library(forecast)

data(boston) # picking up the boston dataset
boston

head(boston)

class(boston)

frequency(boston) # The cycle of this time series is 12 months in a year

summary(boston)

dim(boston) # [1] 35  2   i.e 35 rows and 2 columns

colnames(boston)  # [1] "nyase" "bse"  

plot(boston)

nrow(boston) # 35 rows
ncol(boston) # 2 columns

bse_ts <- ts((boston[,"bse"]), frequency = 12, start = c(1967,1))
bse_ts
class(bse_ts)

plot(bse_ts)
# From our output, we can see that there is randomness. Trend and seasonal component is not present in this time series.
# there are no predictable patterns and we cannot be sure be sure where the peaks and troughs of the cycles are..
# It looks like a white noise series

# ndiffs: Number of differences required for a stationary series
ndiffs(bse_ts) # how much differencing do we need to do to make it stationary
# to estimate the number of differences required to make a given time series stationary
# the output for ndiffs(bse_ts) is 0. This means that there is no requirement for further differencing because
# is no seasonal component present in our time series and so it is a Stationary time series by default.


tsdisplay(bse_ts) # it plots the original time series but it tells you what the ACF and PACF is
# For a stationary time series, the ACF will drop to zero relatively quickly..
# In time series analysis, Autocorrelation Function (ACF) and the partial autocorrelation function (PACF) 
# plots are essential in providing the modelâ€™s orders such as p for AR and q for MA to select the best model for forecasting.

# The two blue dash lines represent the significant threshold levels. 
# Anything that spikes over these two lines reveals the significant correlations.
# When looking at ACF plot, we ignore the long spike at lag 0 (pointed by the blue arrow). For PACF, the line usually starts at 1.


auto_fit <- auto.arima(bse_ts, trace=TRUE, stepwise = FALSE)
# Best model: ARIMA(1,0,0)            with non-zero mean 
auto_fit

# Series: bse_ts 
# ARIMA(p,d,q) 
# According to auto ARIMA, the best model is ARIMA(1,0,0) (first-order autoregressive model )where,
# p = 1, d = 0, q = 0

# p is the number of autoregressive terms,
# d is the number of nonseasonal differences needed for stationarity, and
# q is the number of lagged forecast errors in the prediction equation.


# Coefficients:
#  ar1      mean
# 0.7712  117.3200
# s.e.  0.1009   19.9928

# sigma^2 = 917.9:  log likelihood = -168.47
# AIC=342.94   AICc=343.72   BIC=347.61

attributes(auto_fit) # [1] "forecast_ARIMA" "ARIMA"          "Arima"    

checkresiduals(auto_fit)

# Forecasts from ARIMA(1,0,0) with non-zero mean
plot(forecast(auto_fit,h=5,level=c(99.5)))

#Residual Analysis
Acf(auto_fit$residuals)
Box.test(residuals(auto_fit), lag=20, type="Ljung")
# Box-Ljung test

# data:  residuals(auto_fit)
# X-squared = 24.665, df = 20, p-value = 0.2145

# X-squared (Q) is 24.665 and p-value is 0.2145. If p-value is greater than 0.05 then we accept the null hypothesis and the data values are independent.
# p-value is the probability value for a test

plot.ts(residuals(auto_fit))

hist(auto_fit$residuals) # we can see that there is an outlier (greater than 100).. and rest of the data is equally distributed..

# Diagnostic Plots for Time-Series Fits
tsdiag(auto_fit)

# we can see from the plot of p values for Ljung-Bpx statistic, that the p-values are above the significant threshold
# limit, i.e above 0.05 and so we can accept the null hypothesis.








```

